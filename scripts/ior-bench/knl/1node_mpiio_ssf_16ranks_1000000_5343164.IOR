IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Mon Jun 12 17:47:45 2017
Command line used: /global/cscratch1/sd/fbench/nersc-ior-bench/ior-bench/scripts/ior-bench/knl/../IOR -w -a MPIIO -c -C -H -g -k -b 1000000 -t 1000000 -s 12884 -o /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164/IOR_file -v
Machine: Linux nid09046
Start time skew across all tasks: 0.00 sec

Test 0 started: Mon Jun 12 17:47:45 2017
Path: /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164
FS: 27719.5 TiB   Used FS: 63.7%   Inodes: 5955.2 Mi   Used Inodes: 22.9%
Participating tasks: 16
Using reorderTasks '-C' (expecting block, not cyclic, task assignment)
Summary:
	api                = MPIIO (version=3, subversion=1)
	test filename      = /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164/IOR_file
	access             = single-shared-file, collective
	pattern            = strided (12884 segments)
	ordering in a file = sequential offsets
	ordering inter file= constant task offsets = 1
	clients            = 16 (16 per node)
	repetitions        = 1
	xfersize           = 1000000 bytes
	blocksize          = 1000000 bytes
	aggregate filesize = 191.99 GiB

access    bw(MiB/s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ---------- ---------  --------   --------   --------   --------   ----

hints passed to MPI_File_open() {
	romio_cb_read = disable
	romio_cb_write = disable
}

hints returned from opened file {
	cb_buffer_size = 16777216
	romio_cb_read = disable
	romio_cb_write = disable
	cb_nodes = 16
	cb_align = 2
	romio_no_indep_rw = false
	romio_cb_pfr = disable
	romio_cb_fr_types = aar
	romio_cb_fr_alignment = 1
	romio_cb_ds_threshold = 0
	romio_cb_alltoall = automatic
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = disable
	romio_ds_write = disable
	striping_factor = 16
	striping_unit = 1048576
	romio_lustre_start_iodevice = 0
	direct_io = false
	aggregator_placement_stride = -1
	abort_on_rw_error = disable
	cb_config_list = *:*
	romio_filesystem_type = CRAY ADIO:
}
write     493.91     976.56     976.56     0.169160   397.84     0.019865   398.03     0   

Max Write: 493.91 MiB/sec (517.91 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev    Mean(s) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggsize API RefNum
write         493.91     493.91     493.91       0.00  398.03324 0 16 16 1 0 1 1 0 0 12884 1000000 1000000 206144000000 MPIIO 0

Finished: Mon Jun 12 17:54:23 2017
IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Mon Jun 12 17:54:44 2017
Command line used: /global/cscratch1/sd/fbench/nersc-ior-bench/ior-bench/scripts/ior-bench/knl/../IOR -r -a MPIIO -c -C -H -g -k -b 1000000 -t 1000000 -s 12884 -o /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164/IOR_file -v
Machine: Linux nid09046
Start time skew across all tasks: 0.00 sec

Test 0 started: Mon Jun 12 17:54:44 2017
Path: /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164
FS: 27719.5 TiB   Used FS: 63.8%   Inodes: 5955.2 Mi   Used Inodes: 22.9%
Participating tasks: 16
Using reorderTasks '-C' (expecting block, not cyclic, task assignment)
Summary:
	api                = MPIIO (version=3, subversion=1)
	test filename      = /global/cscratch1/sd/fbench/IOR_REGULAR/1node_mpiio_ssf/5343164/IOR_file
	access             = single-shared-file, collective
	pattern            = strided (12884 segments)
	ordering in a file = sequential offsets
	ordering inter file= constant task offsets = 1
	clients            = 16 (16 per node)
	repetitions        = 1
	xfersize           = 1000000 bytes
	blocksize          = 1000000 bytes
	aggregate filesize = 191.99 GiB
